{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, RepeatVector, TimeDistributed, Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import os\n",
    "import sys\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load source (English)\n",
    "source_path = 'data/training/europarl-v7.fr-en.en'\n",
    "f = open(source_path, 'r')\n",
    "X_data = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption of the session\n",
      "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n"
     ]
    }
   ],
   "source": [
    "for sentence in X_data.split('\\n')[:5]:\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load target (French)\n",
    "target_path = 'data/training/europarl-v7.fr-en.fr'\n",
    "f = open(target_path, 'r')\n",
    "y_data = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprise de la session\n",
      "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
      "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
      "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.\n",
      "En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n"
     ]
    }
   ],
   "source": [
    "for sentence in y_data.split('\\n')[:5]:\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences and sentences into words\n",
    "X = [text_to_word_sequence(sentence) for sentence in X_data.split('\\n')]\n",
    "y = [text_to_word_sequence(sentence) for sentence in y_data.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X) == len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentence lengths\n",
    "X_len = [len(sentence) for sentence in X]\n",
    "y_len = [len(sentence) for sentence in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 0-length sentences\n",
    "X_empty_ix = np.where(np.array(X_len) == 0)[0]\n",
    "y_empty_ix = np.where(np.array(y_len) == 0)[0]\n",
    "empty_ix = np.union1d(X_empty_ix, y_empty_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.delete(np.array(X), list(empty_ix))\n",
    "y2 = np.delete(np.array(y), list(empty_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X2) == len(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sentence lengths\n",
    "X2_len = [len(sentence) for sentence in X2]\n",
    "y2_len = [len(sentence) for sentence in y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   6.,   9.,  11.,  13.,  14.,  16.,  18.,  19.,  21.,  22.,\n",
       "        24.,  26.,  28.,  30.,  33.,  36.,  39.,  44.,  53.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine sentence lengths\n",
    "np.percentile(X2_len, np.arange(0, 100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   7.,   9.,  11.,  13.,  15.,  17.,  18.,  20.,  22.,  23.,\n",
       "        25.,  27.,  29.,  32.,  34.,  37.,  41.,  47.,  56.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(y2_len, np.arange(0, 100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cap the length at 50 for both input and output\n",
    "max_len = 50\n",
    "X_too_long_ix = np.where(np.array(X2_len) > max_len)[0]\n",
    "y_too_long_ix = np.where(np.array(y2_len) > max_len)[0]\n",
    "too_long_ix = np.union1d(X_too_long_ix, y_too_long_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = np.delete(np.array(X2), list(too_long_ix))\n",
    "y3 = np.delete(np.array(y2), list(too_long_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(X3) == len(y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All looks good\n",
    "X, y = X3, y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use 100000 sentences to train (due to time and budget constraint...)\n",
    "np.random.seed(123456)\n",
    "ix = np.arange(len(X))\n",
    "np.random.shuffle(ix)\n",
    "\n",
    "X_small, y_small = X[ix[:100000]], y[ix[:100000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word-to-index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_id_mapping(data, max_vocab_size = 20000):\n",
    "    counter = collections.Counter(np.hstack(data))\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # Pick the most common ones\n",
    "    count_pairs = count_pairs[:max_vocab_size]\n",
    "\n",
    "    # Add 'ZERO' and 'UNK'\n",
    "    # It is important to add 'ZERO' in the beginning\n",
    "    # to make sure zero padding does not interfere with existing words\n",
    "    count_pairs.insert(0, ('ZERO', 0))\n",
    "    count_pairs.append(('UNK', 0))\n",
    "\n",
    "    # Create mapping for both directions\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    id_to_word = dict(zip(range(len(words)), words))\n",
    "    \n",
    "    # Map words to indexes\n",
    "    data_id = [[word_to_id[word] if word in word_to_id else word_to_id['UNK'] for word in sentence] for sentence in data]\n",
    "    \n",
    "    return word_to_id, id_to_word, data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_to_id, X_id_to_word, X_id = create_word_to_id_mapping(X_small)\n",
    "y_word_to_id, y_id_to_word, y_id = create_word_to_id_mapping(y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 20002 20002\n"
     ]
    }
   ],
   "source": [
    "print len(X_id), len(y_id), len(X_word_to_id), len(y_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we need a new initiative from the commission on this\n"
     ]
    }
   ],
   "source": [
    "# Print the first pair as an example\n",
    "print ' '.join([X_id_to_word[i] for i in X_id[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il nous faut une nouvelle initiative de la commission à ce sujet\n"
     ]
    }
   ],
   "source": [
    "print ' '.join([y_id_to_word[i] for i in y_id[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad zeros to make sentences equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_id_padded = pad_sequences(X_id, maxlen=max_len, padding='post')\n",
    "y_id_padded = pad_sequences(y_id, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse input sequence order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_id_padded = np.array([sentence[::-1] for sentence in X_id_padded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sentences(sentences, vocab_size):\n",
    "    sentences_vectorized = np.zeros((sentences.shape[0], sentences.shape[1], vocab_size))\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            sentences_vectorized[i, j, word] = 1\n",
    "\n",
    "    return sentences_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embedding layer\n",
    "X_vocab_size = len(X_id_to_word)\n",
    "hidden_size = 1024\n",
    "\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=X_vocab_size,\n",
    "        output_dim=hidden_size,\n",
    "        input_length=max_len,\n",
    "        mask_zero=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LSTM layer\n",
    "model.add(LSTM(hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the last output of the LSTM layer to the size of the decoder input\n",
    "model.add(RepeatVector(max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create decoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stack LSTM layers\n",
    "num_layers = 3\n",
    "\n",
    "for _ in range(num_layers):\n",
    "    model.add(LSTM(hidden_size, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add dense layer to convert the LSTM output to the shape of target labels\n",
    "y_vocab_size = len(y_id_to_word)\n",
    "\n",
    "model.add(TimeDistributed(Dense(y_vocab_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, add softmax to convert output to probabilities\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 1024)          20482048  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1024)              8392704   \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 50, 1024)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 20002)         20502050  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50, 20002)         0         \n",
      "=================================================================\n",
      "Total params: 74,554,914\n",
      "Trainable params: 74,554,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "save_path = 'output'\n",
    "\n",
    "# Due to memory limit, which affects vectorization, only train 1000 sequences at a time\n",
    "seq_per_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define and process test sentences to translate\n",
    "sentences_to_translate = [\n",
    "    'i like learning french because i am interested in the french culture',\n",
    "    'who ate my cheese that i bought from the market yesterday'\n",
    "]\n",
    "\n",
    "sentences_to_translate_words = [text_to_word_sequence(sentence) for sentence in sentences_to_translate]\n",
    "sentences_to_translate_id = [[X_word_to_id[word] for word in sentence] for sentence in sentences_to_translate_words]\n",
    "\n",
    "sentences_to_translate_id_padded = pad_sequences(sentences_to_translate_id, maxlen=max_len, padding='post')\n",
    "sentences_to_translate_id_padded = [sentence[::-1] for sentence in sentences_to_translate_id_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect all output to a file\n",
    "# First, save the default output\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "# Train model `epochs` times\n",
    "for i in range(epochs):\n",
    "    # Redirect output to a file\n",
    "    log_file_path = 'log_file_epoch_' + str(i) + '.txt'\n",
    "    f = open(os.path.join(save_path, log_file_path), 'w')\n",
    "    sys.stdout = f\n",
    "    \n",
    "    # Shuffle the training data every epoch to avoid local minima\n",
    "    np.random.seed(i)\n",
    "    ix = np.arange(len(X_id_padded))\n",
    "    np.random.shuffle(ix)\n",
    "    \n",
    "    X_id_padded = X_id_padded[ix]\n",
    "    X_id_padded = X_id_padded[ix]\n",
    "    \n",
    "    for j in range(len(X_id_padded) / seq_per_iter):        \n",
    "        # Slice input data (due to memory constraint)\n",
    "        start = j * seq_per_iter\n",
    "        end = min(((j + 1) * seq_per_iter), len(X_id_padded))\n",
    "        print 'Training sequences', round(1.0 * start / len(X_id_padded) * 100, 2), '% to', round(1.0 * end / len(X_id_padded) * 100, 2), '%'\n",
    "        \n",
    "        X_id_padded_tmp = np.array(X_id_padded[start:end])\n",
    "        y_id_padded_tmp = np.array(y_id_padded[start:end])\n",
    "        y_id_padded_tmp_vectorized = vectorize_sentences(y_id_padded_tmp, y_vocab_size)\n",
    "        \n",
    "        # Fit model\n",
    "        model.fit(X_id_padded_tmp, y_id_padded_tmp_vectorized, batch_size=100, epochs=1, verbose=2)\n",
    "    \n",
    "    # Save weights\n",
    "    model.save_weights(os.path.join(save_path, 'checkpoint_epoch_{}.hdf5'.format(i)))\n",
    "    \n",
    "    # Apply model to test sentences to translate\n",
    "    predictions = model.predict(np.array(sentences_to_translate_id_padded))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    predictions_in_words = [' '.join([y_id_to_word[p] for p in prediction if p > 0]) for prediction in predictions]\n",
    "    for k, p in enumerate(predictions_in_words):\n",
    "        print 'Translation of', sentences_to_translate[k], ':', p\n",
    "\n",
    "    f.close()\n",
    "\n",
    "# Restore default output\n",
    "sys.stdout = orig_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = orig_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f4639b798d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
